{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from math import floor\n",
    "from os import listdir\n",
    "from numpy.linalg import inv\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# from sklearn.utils.extmath import randomized_svd\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "# import pandas as pd\n",
    "# import warnings\n",
    "\n",
    "# supress warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"pandas\", lineno=570)\n",
    "\n",
    "# custom tokenizer with stemmer \n",
    "class PorterTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.pt = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.pt.stem(t) for t in RegexpTokenizer(r'(?u)\\b\\w\\w+\\b').tokenize(doc)]\n",
    "\n",
    "# read each file into separate string\n",
    "\n",
    "def read_docs(art_path):\n",
    "    file_list = [name for name in listdir(art_path)]\n",
    "    doc_list = []\n",
    "    for file in file_list:\n",
    "        with open(art_path + file, 'r') as myfile:\n",
    "            doc_list.append(myfile.read())\n",
    "    return file_list, doc_list\n",
    "\n",
    "# vectorize strings using tf-idf vectorizer\n",
    "\n",
    "def tfidf_transform(doc_list):\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=1, stop_words='english', tokenizer=PorterTokenizer(), smooth_idf=True)\n",
    "    tfidf_trans_matrix = tfidf_vectorizer.fit_transform(doc_list)\n",
    "    return tfidf_trans_matrix, list(tfidf_vectorizer.vocabulary_.keys())\n",
    "\n",
    "# perform low rank approximation with given multiplier\n",
    "\n",
    "def lra_mult(tfidf_matrix, mult):\n",
    "    (_, n_org) = tfidf_matrix.shape\n",
    "    n_comp = floor(n_org * mult)\n",
    "    if n_comp < 2:\n",
    "        n_comp = 2\n",
    "    svd = TruncatedSVD(n_comp, \"arpack\")\n",
    "    reduced_m = svd.fit_transform(tfidf_matrix)\n",
    "    return reduced_m\n",
    "\n",
    "def lra(tfidf_matrix, n_comp):\n",
    "    if n_comp < 2:\n",
    "        n_comp = 2\n",
    "    svd = TruncatedSVD(n_comp, \"arpack\")\n",
    "    reduced_m = svd.fit_transform(tfidf_matrix)\n",
    "    return reduced_m\n",
    "\n",
    "# save info\n",
    "\n",
    "def save_matrix(path, tfidf_trans_matrix):\n",
    "    save_npz(file=path+'tfidf_matrix', matrix=tfidf_trans_matrix)\n",
    "    \n",
    "def save_voc(path, voc):    \n",
    "    with open(path+'vocabulary', 'w+') as voc_file:\n",
    "        for item in voc:\n",
    "            voc_file.write(\"%s\\n\" % item)\n",
    "\n",
    "def save_file_list(path, file_list):\n",
    "    with open(path+'file_list', 'w+') as file_list_file:\n",
    "        for item in file_list:\n",
    "            file_list_file.write(\"%s\\n\" % item)\n",
    "            \n",
    "def save_info(path, tfidf_trans_matrix, file_list, voc):\n",
    "    save_matrix(path, tfidf_trans_matrix)\n",
    "    save_voc(path, voc)\n",
    "    save_file_list(path, file_list)\n",
    "\n",
    "# load saved info\n",
    "\n",
    "def load_matrix(path, matrix_filename):\n",
    "    tfidf_trans_matrix = load_npz(file=path+matrix_filename+'.npz')\n",
    "    return tfidf_trans_matrix\n",
    "\n",
    "def load_file_list(path, file_list_filename):\n",
    "    file_list = []\n",
    "    with open(path+file_list_filename, 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            line = line.rstrip('\\n')\n",
    "            file_list.append(line)\n",
    "    return file_list\n",
    "\n",
    "def load_voc(path, voc_filename):\n",
    "    voc = []\n",
    "    with open(voc_filename, 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            line = line.rstrip('\\n')\n",
    "            voc.append(line)\n",
    "    return voc\n",
    "\n",
    "def load_info(path, matrix_filename, file_list_filename, voc_filename):\n",
    "    tfidf_trans_matrix = load_matrix(path, matrix_filename)\n",
    "    file_list = load_file_list(path, file_list_filename)\n",
    "    voc = load_voc(path, voc_filename)\n",
    "    return tfidf_trans_matrix, file_list, voc\n",
    "\n",
    "# search\n",
    "\n",
    "def search(search_str, n, tfidf_matrix, voc):\n",
    "    tfidf_search_vectorizer = TfidfVectorizer(vocabulary=voc, stop_words='english', tokenizer=PorterTokenizer(), smooth_idf=True)\n",
    "    tfidf_search_matrix = tfidf_search_vectorizer.fit_transform(search_str)\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix, tfidf_search_matrix)\n",
    "    a = [i[0] for i in sorted(enumerate(list(sim_matrix[:,0])), key=lambda x:x[1], reverse=True)]\n",
    "    res = [file_list[i] for i in a]\n",
    "    return res[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "art_path = r'/home/yurii/projects/mownit2/lab6/articles/'\n",
    "path = r'/home/yurii/projects/mownit2/lab6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create\n",
    "\n",
    "file_list, doc_list = read_docs(art_path)\n",
    "tfidf_matrix, voc = tfidf_transform(doc_list)\n",
    "save_info(path, tfidf_matrix, file_list, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "tfidf_matrix, file_list, voc = load_info(path, 'tfidf_matrix', 'file_list', 'vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cannon', 'Peveril Castle', 'Albertosaurus', 'Science and technology of the Song dynasty', 'Jupiter trojan']\n"
     ]
    }
   ],
   "source": [
    "query_str = [\"war arms\"]\n",
    "result_list = search(search_str, 5, tfidf_matrix, voc)\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_str = [\"war arms\"]\n",
    "tfidf_query_vectorizer = TfidfVectorizer(vocabulary=voc, stop_words='english', tokenizer=PorterTokenizer(), smooth_idf=True)\n",
    "tfidf_query_matrix = tfidf_search_vectorizer.fit_transform(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=100)\n",
    "tfidf_matrix_t = lsa.fit_transform(tfidf_matrix)\n",
    "tfidf_matrix_t = Normalizer(copy=False).fit_transform(tfidf_matrix_t)\n",
    "query = lsa.transform(tfidf_query_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Castle', 'Warwick Castle', 'Rochester Castle', 'Bodiam Castle', 'Peveril Castle']\n"
     ]
    }
   ],
   "source": [
    "sim_matrix = cosine_similarity(tfidf_matrix_t, query)\n",
    "a = [i[0] for i in sorted(enumerate(list(sim_matrix[:,0])), key=lambda x:x[1], reverse=True)]\n",
    "res = [file_list[i] for i in a]\n",
    "print(res[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lsa_learn(tfidf_matrix, n_comp):\n",
    "    lsa = TruncatedSVD(n_components=n_comp)\n",
    "    tfidf_matrix_t = lsa.fit_transform(tfidf_matrix)\n",
    "    tfidf_matrix_t = Normalizer(copy=False).fit_transform(tfidf_matrix_t)\n",
    "    return lsa, tfidf_matrix_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lsa_search(query_str, n, tfidf_matrix_t, voc, lsa=None, n_comp=100):\n",
    "    tfidf_query_vectorizer = TfidfVectorizer(vocabulary=voc, stop_words='english', tokenizer=PorterTokenizer(), smooth_idf=True)\n",
    "    tfidf_query_matrix = tfidf_search_vectorizer.fit_transform(query_str)\n",
    "    if lsa == None:\n",
    "        lsa, tfidf_matrix_t = lsa_learn(tfidf_matrix_t, n_comp)    \n",
    "    query = lsa.transform(tfidf_query_matrix)\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix_t, query)\n",
    "    a = [i[0] for i in sorted(enumerate(list(sim_matrix[:,0])), key=lambda x:x[1], reverse=True)]\n",
    "    res = [file_list[i] for i in a]\n",
    "    return res[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsa, tfidf_matrix_t = lsa_learn(tfidf_matrix, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components must be < n_features; got 200 >= 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ec0d4a17fa18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrix_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_comp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-42aefa7e67cd>\u001b[0m in \u001b[0;36mlsa_search\u001b[0;34m(query_str, n, tfidf_matrix_t, voc, lsa, n_comp)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtfidf_query_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_search_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlsa\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrix_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_comp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_query_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msim_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-698ea9cfc0ea>\u001b[0m in \u001b[0;36mlsa_learn\u001b[0;34m(tfidf_matrix, n_comp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlsa_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_comp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_comp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtfidf_matrix_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtfidf_matrix_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrix_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 raise ValueError(\"n_components must be < n_features;\"\n\u001b[0;32m--> 175\u001b[0;31m                                  \" got %d >= %d\" % (k, n_features))\n\u001b[0m\u001b[1;32m    176\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_components must be < n_features; got 200 >= 100"
     ]
    }
   ],
   "source": [
    "res = lsa_search(query_str, 5, tfidf_matrix_t, voc, n_comp=200)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process generated files\n",
    "abs_path = r'/home/yurii/projects/mownit2/lab6/outl/AB/'\n",
    "cur_path = r'/home/yurii/projects/mownit2/lab6/a/'\n",
    "for filename in listdir(abs_path):\n",
    "    with open(abs_path + filename, 'r') as myfile:\n",
    "        s = myfile.read()\n",
    "        p = re.compile('<doc[^>]*>([^<]*)<\\/doc>')\n",
    "        l = p.findall(s)\n",
    "        for i in range(0,len(l)):\n",
    "            p = re.compile('Notes\\n|Footnotes\\n|Bibliography\\n|References\\n|External links\\n|Further reading\\n')\n",
    "            l[i] = p.sub(r'', l[i])\n",
    "            p = re.compile('\\n{3,}')\n",
    "            l[i] = p.sub(r'\\n', l[i])\n",
    "            if l[i][0] == '\\n':\n",
    "                l[i] = l[i][1:]\n",
    "            title = l[i].splitlines()[0]\n",
    "            title = title.replace(r'/', ' ')\n",
    "            with open(cur_path + title, 'w+') as myfile2:\n",
    "                myfile2.write(l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector = CountVectorizer(min_df=1, stop_words='english', tokenizer=PorterTokenizer())\n",
    "# trans_vect = vector.fit_transform(doc_list)\n",
    "# dt = pd.DataFrame(trans_vect.toarray(), index=file_list, columns=vector.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example = [\n",
    "#     'In mathematics, 1 − 2 + 3 − 4 + ··· is the infinite series whose terms are the successive positive integers, given alternating signs. Using sigma summation notation the sum of the first \"m\" terms of the series can be expressed as',\n",
    "#     'The infinite series diverges, meaning that its sequence of partial sums, , does not tend towards any finite limit. Nonetheless, in the mid-18th century, Leonhard Euler wrote what he admitted to be a paradoxical equation:',\n",
    "#     \"A rigorous explanation of this equation would not arrive until much later. Starting in 1890, Ernesto Cesàro, Émile Borel and others investigated well-defined methods to assign generalized sums to divergent series—including new interpretations of Euler's attempts.\"\n",
    "# ]\n",
    "\n",
    "# vect1 = CountVectorizer(min_df=1, stop_words='english', tokenizer=PorterTokenizer())\n",
    "# # vect2 = CountVectorizer(min_df=1, stop_words='english')\n",
    "# dtm1 = vect1.fit_transform(example)\n",
    "# # dtm2 = vect2.fit_transform(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(dtm1.toarray(), index=example, columns=vect1.get_feature_names()).head(10)\n",
    "# vect1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(dtm2.toarray(), index=example, columns=vect2.get_feature_names()).head(10)\n",
    "# vect2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dict_list = [col.OrderedDict([]) for i in range(0,len(file_list))]\n",
    "\n",
    "# def read_words(words_file):\n",
    "#     return [word for line in open(words_file, 'r') for word in line.split()]\n",
    "\n",
    "# file_name = art_path + r'0'\n",
    "# wl = read_words(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
