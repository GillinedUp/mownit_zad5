{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-6ebe3ccc1694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.matrix'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from os import listdir\n",
    "from numpy.linalg import inv\n",
    "from numpy.matrix import transpose\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "# import pandas as pd\n",
    "# import warnings\n",
    "\n",
    "# supress warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"pandas\", lineno=570)\n",
    "\n",
    "# custom tokenizer with stemmer \n",
    "class PorterTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.pt = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.pt.stem(t) for t in RegexpTokenizer(r'(?u)\\b\\w\\w+\\b').tokenize(doc)]\n",
    "\n",
    "# read each file into separate string\n",
    "\n",
    "def read_docs(art_path):\n",
    "    file_list = [name for name in os.listdir(art_path)]\n",
    "    doc_list = []\n",
    "    for file in file_list:\n",
    "        with open(art_path + file, 'r') as myfile:\n",
    "            doc_list.append(myfile.read())\n",
    "    return file_list, doc_list\n",
    "\n",
    "# vectorize strings using tf-idf vectorizer\n",
    "\n",
    "def tfidf_transform(doc_list):\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=1, stop_words='english', tokenizer=PorterTokenizer(), smooth_idf=True)\n",
    "    tfidf_trans_matrix = tfidf_vectorizer.fit_transform(doc_list)\n",
    "    return tfidf_trans_matrix, list(tfidf_vectorizer.vocabulary_.keys())\n",
    "\n",
    "# perform low rank approximation with given multiplier\n",
    "\n",
    "def lra_mult(tfidf_matrix, mult):\n",
    "    (_, n_org) = tfidf_matrix.shape\n",
    "    n_comp = floor(n_org * mult)\n",
    "    if n_comp < 2:\n",
    "        n_comp = 2\n",
    "    svd = TruncatedSVD(n_comp, \"arpack\")\n",
    "    reduced_m = svd.fit_transform(tfidf_matrix)\n",
    "    return reduced_m\n",
    "\n",
    "def lra(tfidf_matrix, n_comp):\n",
    "    if n_comp < 2:\n",
    "        n_comp = 2\n",
    "    svd = TruncatedSVD(n_comp, \"arpack\")\n",
    "    reduced_m = svd.fit_transform(tfidf_matrix)\n",
    "    return reduced_m\n",
    "\n",
    "# save info\n",
    "\n",
    "def save_matrix(path, tfidf_trans_matrix):\n",
    "    save_npz(file=path+'tfidf_matrix', matrix=tfidf_trans_matrix)\n",
    "    \n",
    "def save_voc(path, voc):    \n",
    "    with open(path+'vocabulary', 'w+') as voc_file:\n",
    "        for item in voc:\n",
    "            voc_file.write(\"%s\\n\" % item)\n",
    "\n",
    "def save_file_list(path, file_list):\n",
    "    with open(path+'file_list', 'w+') as file_list_file:\n",
    "        for item in file_list:\n",
    "            file_list_file.write(\"%s\\n\" % item)\n",
    "            \n",
    "def save_info(path, tfidf_trans_matrix, file_list, voc):\n",
    "    save_matrix(path, tfidf_trans_matrix)\n",
    "    save_voc(path, voc)\n",
    "    save_file_list(path, file_list)\n",
    "\n",
    "# load saved info\n",
    "\n",
    "def load_matrix(path, matrix_filename):\n",
    "    tfidf_trans_matrix = load_npz(file=path+matrix_filename+'.npz')\n",
    "    return tfidf_trans_matrix\n",
    "\n",
    "def load_file_list(path, file_list_filename):\n",
    "    file_list = []\n",
    "    with open(path+file_list_filename, 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            line = line.rstrip('\\n')\n",
    "            file_list.append(line)\n",
    "    return file_list\n",
    "\n",
    "def load_voc(path, voc_filename):\n",
    "    voc = []\n",
    "    with open(voc_filename, 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            line = line.rstrip('\\n')\n",
    "            voc.append(line)\n",
    "    return voc\n",
    "\n",
    "def load_info(path, matrix_filename, file_list_filename, voc_filename):\n",
    "    tfidf_trans_matrix = load_matrix(path, matrix_filename)\n",
    "    file_list = load_file_list(path, file_list_filename)\n",
    "    voc = load_voc(path, voc_filename)\n",
    "    return tfidf_trans_matrix, file_list, voc\n",
    "\n",
    "# search\n",
    "\n",
    "def search(search_str, n, tfidf_matrix, voc):\n",
    "    tfidf_search_vectorizer = TfidfVectorizer(vocabulary=voc, stop_words='english', tokenizer=PorterTokenizer(), smooth_idf=True)\n",
    "    tfidf_search_matrix = tfidf_search_vectorizer.fit_transform(search_str)\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix, tfidf_search_matrix)\n",
    "    a = [i[0] for i in sorted(enumerate(list(sim_matrix[:,0])), key=lambda x:x[1], reverse=True)]\n",
    "    res = [file_list[i] for i in a]\n",
    "    return res[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "art_path = r'/home/yurii/projects/mownit2/lab6/articles/'\n",
    "path = r'/home/yurii/projects/mownit2/lab6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create\n",
    "\n",
    "file_list, doc_list = read_docs(art_path)\n",
    "tfidf_matrix, voc = tfidf_transform(doc_list)\n",
    "save_info(path, tfidf_matrix, file_list, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "tfidf_matrix, file_list, voc = load_info(path, 'tfidf_matrix', 'file_list', 'vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U, Sigma, VT = randomized_svd(tfidf_matrix, n_components=100, n_iter=5, random_state=None)\n",
    "inv_sigma = np.reciprocal(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_str = [\"war arms\"]\n",
    "tfidf_search_vectorizer = TfidfVectorizer(vocabulary=voc, stop_words='english', tokenizer=PorterTokenizer(), smooth_idf=True)\n",
    "tfidf_search_matrix = tfidf_search_vectorizer.fit_transform(search_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UT = np.matrix.transpose(U)\n",
    "q = tfidf_search_matrix.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma_d = np.diag(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (100,4516) and (168825,) not aligned: 4516 (dim 1) != 168825 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-568ade4ce175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: shapes (100,4516) and (168825,) not aligned: 4516 (dim 1) != 168825 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(np.dot(sigma_d, UT), q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cannon', 'Peveril Castle', 'Albertosaurus', 'Science and technology of the Song dynasty', 'Jupiter trojan']\n"
     ]
    }
   ],
   "source": [
    "search_str = [\"war arms\"]\n",
    "result_list = search(search_str, 5, tfidf_matrix, voc)\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process generated files\n",
    "abs_path = r'/home/yurii/projects/mownit2/lab6/outl/AB/'\n",
    "cur_path = r'/home/yurii/projects/mownit2/lab6/a/'\n",
    "for filename in os.listdir(abs_path):\n",
    "    with open(abs_path + filename, 'r') as myfile:\n",
    "        s = myfile.read()\n",
    "        p = re.compile('<doc[^>]*>([^<]*)<\\/doc>')\n",
    "        l = p.findall(s)\n",
    "        for i in range(0,len(l)):\n",
    "            p = re.compile('Notes\\n|Footnotes\\n|Bibliography\\n|References\\n|External links\\n|Further reading\\n')\n",
    "            l[i] = p.sub(r'', l[i])\n",
    "            p = re.compile('\\n{3,}')\n",
    "            l[i] = p.sub(r'\\n', l[i])\n",
    "            if l[i][0] == '\\n':\n",
    "                l[i] = l[i][1:]\n",
    "            title = l[i].splitlines()[0]\n",
    "            title = title.replace(r'/', ' ')\n",
    "            with open(cur_path + title, 'w+') as myfile2:\n",
    "                myfile2.write(l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector = CountVectorizer(min_df=1, stop_words='english', tokenizer=PorterTokenizer())\n",
    "# trans_vect = vector.fit_transform(doc_list)\n",
    "# dt = pd.DataFrame(trans_vect.toarray(), index=file_list, columns=vector.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example = [\n",
    "#     'In mathematics, 1 − 2 + 3 − 4 + ··· is the infinite series whose terms are the successive positive integers, given alternating signs. Using sigma summation notation the sum of the first \"m\" terms of the series can be expressed as',\n",
    "#     'The infinite series diverges, meaning that its sequence of partial sums, , does not tend towards any finite limit. Nonetheless, in the mid-18th century, Leonhard Euler wrote what he admitted to be a paradoxical equation:',\n",
    "#     \"A rigorous explanation of this equation would not arrive until much later. Starting in 1890, Ernesto Cesàro, Émile Borel and others investigated well-defined methods to assign generalized sums to divergent series—including new interpretations of Euler's attempts.\"\n",
    "# ]\n",
    "\n",
    "# vect1 = CountVectorizer(min_df=1, stop_words='english', tokenizer=PorterTokenizer())\n",
    "# # vect2 = CountVectorizer(min_df=1, stop_words='english')\n",
    "# dtm1 = vect1.fit_transform(example)\n",
    "# # dtm2 = vect2.fit_transform(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(dtm1.toarray(), index=example, columns=vect1.get_feature_names()).head(10)\n",
    "# vect1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(dtm2.toarray(), index=example, columns=vect2.get_feature_names()).head(10)\n",
    "# vect2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dict_list = [col.OrderedDict([]) for i in range(0,len(file_list))]\n",
    "\n",
    "# def read_words(words_file):\n",
    "#     return [word for line in open(words_file, 'r') for word in line.split()]\n",
    "\n",
    "# file_name = art_path + r'0'\n",
    "# wl = read_words(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
