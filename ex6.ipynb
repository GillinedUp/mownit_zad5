{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from math import floor\n",
    "from os import listdir\n",
    "from numpy.linalg import inv\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# from sklearn.utils.extmath import randomized_svd\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "# import pandas as pd\n",
    "# import warnings\n",
    "\n",
    "# supress warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"pandas\", lineno=570)\n",
    "\n",
    "# custom tokenizer with stemmer \n",
    "class PorterTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.pt = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.pt.stem(t) for t in RegexpTokenizer(r'(?u)\\b\\w\\w+\\b').tokenize(doc)]\n",
    "\n",
    "# read each file into separate string\n",
    "\n",
    "def read_docs(art_path):\n",
    "    file_list = [name for name in listdir(art_path)]\n",
    "    doc_list = []\n",
    "    for file in file_list:\n",
    "        with open(art_path + file, 'r') as myfile:\n",
    "            doc_list.append(myfile.read())\n",
    "    return file_list, doc_list\n",
    "\n",
    "# vectorize strings using tf-idf vectorizer\n",
    "\n",
    "def tfidf_transform(doc_list):\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=1, stop_words='english', tokenizer=PorterTokenizer(), smooth_idf=True)\n",
    "    tfidf_trans_matrix = tfidf_vectorizer.fit_transform(doc_list)\n",
    "    return tfidf_trans_matrix, list(tfidf_vectorizer.vocabulary_.keys())\n",
    "\n",
    "# perform low rank approximation with given multiplier\n",
    "\n",
    "def lra_mult(tfidf_matrix, mult):\n",
    "    (_, n_org) = tfidf_matrix.shape\n",
    "    n_comp = floor(n_org * mult)\n",
    "    if n_comp < 2:\n",
    "        n_comp = 2\n",
    "    svd = TruncatedSVD(n_comp, \"arpack\")\n",
    "    reduced_m = svd.fit_transform(tfidf_matrix)\n",
    "    return reduced_m\n",
    "\n",
    "def lra(tfidf_matrix, n_comp):\n",
    "    if n_comp < 2:\n",
    "        n_comp = 2\n",
    "    svd = TruncatedSVD(n_comp, \"arpack\")\n",
    "    reduced_m = svd.fit_transform(tfidf_matrix)\n",
    "    return reduced_m\n",
    "\n",
    "# save info\n",
    "\n",
    "# def save_matrix(path, filename, tfidf_matrix):\n",
    "#     save_npz(file=path+filename, matrix=tfidf_matrix)\n",
    "    \n",
    "# def save_voc(path, filename, voc):    \n",
    "#     with open(path+filename, 'w+') as voc_file:\n",
    "#         for item in voc:\n",
    "#             voc_file.write(\"%s\\n\" % item)\n",
    "\n",
    "def save_list(path, filename, l):\n",
    "    with open(path+filename, 'w+') as f:\n",
    "        for item in l:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "def save_info(path, tfidf_matrix, file_list, voc, lsa, tfidf_matrix_t):\n",
    "    save_npz(file=path+'tfidf_matrix', matrix=tfidf_matrix)\n",
    "    with open(path+'tfidf_matrix_t', 'wb') as f:    \n",
    "        np.save(f, tfidf_matrix_t)\n",
    "    save_list(path, 'voc', voc)\n",
    "    save_list(path, 'file_list', file_list)\n",
    "    with open('lsa.pickle', 'wb') as f:    \n",
    "        pickle.dump(lsa, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load saved info\n",
    "\n",
    "# def load_matrix(path, matrix_filename):\n",
    "#     tfidf_trans_matrix = load_npz(file=path+matrix_filename+'.npz')\n",
    "#     return tfidf_trans_matrix\n",
    "\n",
    "def load_list(path, filename):\n",
    "    l = []\n",
    "    with open(path+filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            l.append(line)\n",
    "    return l\n",
    "\n",
    "# def load_voc(path, voc_filename):\n",
    "#     voc = []\n",
    "#     with open(voc_filename, 'r') as myfile:\n",
    "#         for line in myfile:\n",
    "#             line = line.rstrip('\\n')\n",
    "#             voc.append(line)\n",
    "#     return voc\n",
    "\n",
    "def load_info(path, matrix_filename, file_list_filename, voc_filename, lsa_filename, matrix_t_filename):\n",
    "    tfidf_matrix = load_npz(file=path+matrix_filename+'.npz')\n",
    "    with open(path+matrix_t_filename, 'rb') as f:    \n",
    "        tfidf_matrix_t = np.load(f)\n",
    "    file_list = load_list(path, file_list_filename)\n",
    "    voc = load_list(path, voc_filename)\n",
    "    with open('lsa.pickle', 'rb') as f:    \n",
    "        lsa = pickle.load(f)\n",
    "    with open(path+matrix_t_filename, 'rb') as f:    \n",
    "        tfidf_matrix_t = np.load(f)    \n",
    "    return tfidf_matrix, file_list, voc, lsa, tfidf_matrix_t\n",
    "\n",
    "# search\n",
    "\n",
    "def search(search_str, n, tfidf_matrix, voc, file_list):\n",
    "    tfidf_search_vectorizer = TfidfVectorizer(vocabulary=voc, stop_words='english', tokenizer=PorterTokenizer(), smooth_idf=True)\n",
    "    tfidf_search_matrix = tfidf_search_vectorizer.fit_transform(search_str)\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix, tfidf_search_matrix)\n",
    "    a = [i[0] for i in sorted(enumerate(list(sim_matrix[:,0])), key=lambda x:x[1], reverse=True)]\n",
    "    res = [file_list[i] for i in a]\n",
    "    return res[:n]\n",
    "\n",
    "def lsa_learn(tfidf_matrix, n_comp):\n",
    "    lsa = TruncatedSVD(n_components=n_comp)\n",
    "    tfidf_matrix_t = lsa.fit_transform(tfidf_matrix)\n",
    "    tfidf_matrix_t = Normalizer(copy=False).fit_transform(tfidf_matrix_t)\n",
    "    return lsa, tfidf_matrix_t\n",
    "\n",
    "def lsa_search(query_str, n, tfidf_matrix_t, voc, lsa, file_list):\n",
    "    tfidf_query_vectorizer = TfidfVectorizer(vocabulary=voc, stop_words='english', tokenizer=PorterTokenizer(), smooth_idf=True)\n",
    "    tfidf_query_matrix = tfidf_query_vectorizer.fit_transform(query_str) \n",
    "    query = lsa.transform(tfidf_query_matrix)\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix_t, query)\n",
    "    a = [i[0] for i in sorted(enumerate(list(sim_matrix[:,0])), key=lambda x:x[1], reverse=True)]\n",
    "    res = [file_list[i] for i in a]\n",
    "    return res[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "art_path = r'/home/yurii/projects/mownit2/lab6/articles/'\n",
    "path = r'/home/yurii/projects/mownit2/lab6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create\n",
    "\n",
    "n_comp = 100\n",
    "\n",
    "file_list, doc_list = read_docs(art_path)\n",
    "tfidf_matrix, voc = tfidf_transform(doc_list)\n",
    "lsa, tfidf_matrix_t = lsa_learn(tfidf_matrix, n_comp)\n",
    "save_info(path, tfidf_matrix, file_list, voc, lsa, tfidf_matrix_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info(path, tfidf_matrix, file_list, voc, lsa, tfidf_matrix_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "\n",
    "tfidf_matrix, file_list, voc, lsa, tfidf_matrix_t = load_info(path, 'tfidf_matrix', 'file_list', 'voc', 'lsa.pickle', 'tfidf_matrix_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_str = [\"hurricane\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HMS Hood', 'Battle of Khafji', 'Ruma Maida', 'Ahalya', 'Eadbald of Kent']\n"
     ]
    }
   ],
   "source": [
    "result_list = search(query_str, 5, tfidf_matrix, voc, file_list)\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsa, tfidf_matrix_t = lsa_learn(tfidf_matrix, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HMS Hood', 'Courageous-class battlecruiser', 'HMS Vanguard (23)', 'HMS Queen Mary', 'St Vincent-class battleship']\n"
     ]
    }
   ],
   "source": [
    "result_list = lsa_search(query_str, 5, tfidf_matrix_t, voc, lsa, file_list)\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = lsa_search(query_str, 5, tfidf_matrix_t, voc, n_comp=200)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process generated files\n",
    "abs_path = r'/home/yurii/projects/mownit2/lab6/outl/AB/'\n",
    "cur_path = r'/home/yurii/projects/mownit2/lab6/a/'\n",
    "for filename in listdir(abs_path):\n",
    "    with open(abs_path + filename, 'r') as myfile:\n",
    "        s = myfile.read()\n",
    "        p = re.compile('<doc[^>]*>([^<]*)<\\/doc>')\n",
    "        l = p.findall(s)\n",
    "        for i in range(0,len(l)):\n",
    "            p = re.compile('Notes\\n|Footnotes\\n|Bibliography\\n|References\\n|External links\\n|Further reading\\n')\n",
    "            l[i] = p.sub(r'', l[i])\n",
    "            p = re.compile('\\n{3,}')\n",
    "            l[i] = p.sub(r'\\n', l[i])\n",
    "            if l[i][0] == '\\n':\n",
    "                l[i] = l[i][1:]\n",
    "            title = l[i].splitlines()[0]\n",
    "            title = title.replace(r'/', ' ')\n",
    "            with open(cur_path + title, 'w+') as myfile2:\n",
    "                myfile2.write(l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector = CountVectorizer(min_df=1, stop_words='english', tokenizer=PorterTokenizer())\n",
    "# trans_vect = vector.fit_transform(doc_list)\n",
    "# dt = pd.DataFrame(trans_vect.toarray(), index=file_list, columns=vector.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example = [\n",
    "#     'In mathematics, 1 − 2 + 3 − 4 + ··· is the infinite series whose terms are the successive positive integers, given alternating signs. Using sigma summation notation the sum of the first \"m\" terms of the series can be expressed as',\n",
    "#     'The infinite series diverges, meaning that its sequence of partial sums, , does not tend towards any finite limit. Nonetheless, in the mid-18th century, Leonhard Euler wrote what he admitted to be a paradoxical equation:',\n",
    "#     \"A rigorous explanation of this equation would not arrive until much later. Starting in 1890, Ernesto Cesàro, Émile Borel and others investigated well-defined methods to assign generalized sums to divergent series—including new interpretations of Euler's attempts.\"\n",
    "# ]\n",
    "\n",
    "# vect1 = CountVectorizer(min_df=1, stop_words='english', tokenizer=PorterTokenizer())\n",
    "# # vect2 = CountVectorizer(min_df=1, stop_words='english')\n",
    "# dtm1 = vect1.fit_transform(example)\n",
    "# # dtm2 = vect2.fit_transform(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(dtm1.toarray(), index=example, columns=vect1.get_feature_names()).head(10)\n",
    "# vect1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(dtm2.toarray(), index=example, columns=vect2.get_feature_names()).head(10)\n",
    "# vect2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dict_list = [col.OrderedDict([]) for i in range(0,len(file_list))]\n",
    "\n",
    "# def read_words(words_file):\n",
    "#     return [word for line in open(words_file, 'r') for word in line.split()]\n",
    "\n",
    "# file_name = art_path + r'0'\n",
    "# wl = read_words(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
